---
title: "Report week 2 Data Science Capstone"
author: "Arnout van der Meijden"
date: "10/7/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Summary
This report is written to show the results of the exploratory data analysis involved and will explain the next planned stepps to create an algorithm and Shiny app. 

## Warning
For easy reading of numbers I have added a dot every three numbers (thousands, millions, etc.). This is common in the Netherlands but not in every country (some countries use the , for this). For example 23.000 is twenty-three thousand. 

## Exploratory analysis
The following explains the exploratory data analysis performed on the News, Twitter and Blog text provided. 
First look at the basics of these three files.

|characters|words|lines|name of file|
|-------:|-------:|-------:|-------:|
|1.010.242|34.365.905|205.243.643|en_US.news.txt|
|899.288|37.333.958|208.623.085|en_US.blogs.txt|
|2.360.148|30.357.171|166.843.164|en_US.twitter.txt|

So they seem to contain about the same amount of words and lines. 
Only the amount of characters is a lot more in the twitter file.

I was expecting to see the following: 

Twitter -> mostly short words, bold language, abbreviations, hashtags, lots of mistakes in spelling
blog -> better written English, more 'difficult' words, less mistakes in spelling
news -> better written English, more 'difficult' words, almost no mistakes in spelling, headlines can be misleading (double meaning).

First step: How much profanity is there in the texts and remove them so the prediction algorithm would not predict any profanity.

|profanity occurrences|unique profanity words|file|
|-------:|-------:|-------:|
|21|15|en_US.news.txt|
|7.436|388|en_US.blogs.txt|
|51.306|1.565|en_US.twitter.txt|

To see which words I have classified as profanity, check the code part. 
It is clear that the twitter contains a lot of profanity. The news almost none and the blogs text some. 

I would also like to know how much of commonly misspelled words are used in these texts.

|misspelled occurrences|unique misspelled words|file|
|-------:|-------:|-------:|
|5|4|en_US.news.txt|
|806|63|en_US.blogs.txt|
|2.374|102|en_US.twitter.txt|

As expected the Twitter text contains the most spelling errors. I will not use the twitter text as source of the prediction algorithm because of the many spelling and typing error and profanity used. In my opinion the other two texts are better suited as basis for the algorithm. 

## Graphs

Next look at some graphs about the texts. 

This treeview below will show the most used words (outside the top 5 in the news section.
The top 5 used words are: the, to, and, a and of)

![Most used words outside top 5 in news](images/Mostusedwordsinnewstreemap.png)


Top 5 in news
|Word|number of occurrences in the news text|
|-------:|-------:|
|the|132.431|
|to|69.144|
|and|66.073|
|a|63.849|
|of|59.052|


Top 5 in blogs
|Word|number of occurrences in the news text|
|-------:|-------:|
|the|1.669.690|
|to|1.055.412|
|and|1.036.007|
|of|868.429|
|a|863.508|

Top 5 of blogs and news have the same 5 words, almost also in rank only number four and five are switched. 

![Most used words outside top 5 in blogs](images/Mostusedwordsinblogs.png)

Top 5 words in twitter

|Word|number of occurrences in the news text|
|-------:|-------:|
|the|842.176|
|to|770.543|
|I||622.536|
|a|576.489|
|you|481.747|

Top 5 of twitter has the same No 1 and 2 as in the other texts (the and to) but the main difference is that I and you are in the top 5 of twitter and not in the other texts.

![Most used words outside top 5 in twitter](images/Mostusedwordsintwitter.png)



## Creating a prediction algorithm and Shiny app

First step in building the algorithm is making sure the input is correct. I will remove profanity (based on .. ). Then then next step is to evaluate if the three texts are the right input. I am expecting that the twitter feed will not very usable (lot of profanity, misspelled words, etc.) so will probably not use this datasource as input for the algorithm. I will perform some other checks and based on these results will remove more data. 
When the data is of high enough quality I will start creating the algorith. 

To build the algorithm I want to use the 2-grams and 3-grams tokens. So based on the texts these are the most used combination of words. So if someone enters the first word, the algorithm will look for the word(s) which are used the most to give these options. I will combine this approach with other predicting techniques to get the best results. 

At this moment I have some ideas how to cope with 'unkown' words but not yet a definite plan how to cope with these words. I will encounter some unforeseen problems. 

For the webapplication my plan is to generate three suggestions for the next word after the fourth letter that is typed, provided I can make the algorithm fast enough. I will also have a button for words shorter than 4 letters or give the user an option to let the app know the user is finished typing and waiting for suggestions. The user gets three suggestions and can choose one by clicking on it. This will add the word to the text the user is typing. The user will get three suggestions for the next word.  


## The code used for the exploratory analysis
```{r, eval=FALSE}

con <- file("C:/Users/ameij/OneDrive/Bureaublad/Capstone project/Coursera-SwiftKey/final/en_US/en_US.news.txt", "r") 
news <- readLines(con, 1010242) ## Read all the lines from the news text
close(con)

library(quanteda)
newstokenized <- tokenize_word(news)

newstokenized2 <- unlist(newstokenized)

newstokenized3 <- as.data.frame(newstokenized2)

patternprof6 <- "ahole|arsehole|asshole|bimbo|bitch|blowjob|bollock|bullshit|cunt|dipshit|ejaculate|faggot|fuck|jackass|nigga|nigger|penis|pussy|slut|whore"

profinnews2 <- grep(patternprof6, newstokenized2, value = TRUE, ignore.case = TRUE)
unique(profinnews2) ## only 15 words

rm(news) ## We don't use these variable anymore
rm(newstokenized) ## We don't use these variable anymore (is about 200 Mb)

## If we do this for blog we get:

## 899288 en_US.blogs.txt lines
con2 <- file("C:/Users/ameij/OneDrive/Bureaublad/Capstone project/Coursera-SwiftKey/final/en_US/en_US.blogs.txt", "r") 
blogs <- readLines(con2, 899288) ## Read all the lines from the blogs text
close(con2)

blogstokenized <- tokenize_word(blogs)

blogstokenized2 <- unlist(blogstokenized)

blogstokenized3 <- as.data.frame(blogstokenized2)

profinblogs2 <- grep(patternprof6, blogstokenized2, value = TRUE, ignore.case = TRUE) ## about 7436
unique(profinblogs2) ## 388 unique words. 

rm(blogs) ## We don't use these variable anymore
rm(blogstokenized) ## We don't use these variable anymore (is about 2,6 Gb)

## If we do this for twitter we get:
## wc -l en_US.twitter.txt

con3 <- file("C:/Users/ameij/OneDrive/Bureaublad/Capstone project/Coursera-SwiftKey/final/en_US/en_US.twitter.txt", "r") 
twitter <- readLines(con3, 2360148) ## Read all the lines from the blogs text
close(con3)

twittertokenized <- tokenize_word(twitter)

twittertokenized2 <- unlist(twittertokenized)

twittertokenized3 <- as.data.frame(twittertokenized2)

profintwitter2 <- grep(patternprof6, twittertokenized2, value = TRUE, ignore.case = TRUE) 
unique(profintwitter2) 

rm(twitter) ## We don't use these variable anymore
rm(twittertokenized) ## We don't use these variable anymore (is about 2,7 Gb)


## Looking for spelling mistakes in the text
## ((source: https://www.learnenglish.de/spelling/commonspellingmistakes.html))
patterncommonmistakes <- "adress|alot|definately|enviroment|expresso|forteen"

misspelledinnews <- grep(patterncommonmistakes, newstokenized2, value = TRUE, ignore.case = TRUE)
unique(misspelledinnews)

misspelledinnblog <- grep(patterncommonmistakes, blogstokenized2, value = TRUE, ignore.case = TRUE) 
unique(misspelledinnblog)

misspelledintwitter <- grep(patterncommonmistakes, twittertokenized2, value = TRUE, ignore.case = TRUE) 
unique(misspelledintwitter)

### Some plots

library(ggplot2)
library(dplyr)
newstokenized3 %>%
  count(newstokenized2, sort = TRUE) %>%
  filter(n < 5000) %>%  ## to filter out space, is, the, it, not, you, etc. 
  filter(n > 2000) %>%
  ggplot(aes(newstokenized2, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()

## Or a treemap with the most used words
Mostusedwordsinnews <- newstokenized3 %>%
  count(newstokenized2, sort = TRUE) %>%
  filter(n < 50000) %>%
  filter(n > 10000)

blogstokenized3 %>%
  count(blogstokenized2, sort = TRUE) %>%
  filter(n < 850000) %>%  ## to filter out space, is, the, it, not, you, etc. 
  filter(n > 200000) %>%
  ggplot(aes(blogstokenized2, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()


Mostusedwordsinblogs2 <- blogstokenized3 %>%
  count(blogstokenized2, sort = TRUE) %>%
  filter(n > 300000)


Mostusedwordsintwitter2 <- twittertokenized3 %>%
  count(twittertokenized2, sort = TRUE) %>%
  filter(n > 450000)

twittertokenized3 %>%
  count(twittertokenized2, sort = TRUE) %>%
  filter(n < 450000) %>%  
  filter(n > 200000) %>%
  ggplot(aes(twittertokenized2, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()


```